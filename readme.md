The project involves processing of data generated from a data generator using Apache Spark
Streaming and Spark SQL. The data is then streamed to Apache Kafka Streaming for publishing and
subscribing the results or producing and consuming from at least three topics. The data is then
stored in a DBMS of choice such as PostgreSQL. Additional tools like Zookeeper can also be used
as required.
The project aims to showcase the use of distributed computing using Apache Spark and Kafka
Streaming to process and analyze data in real-time. Spark SQL queries can be used to carry out
various actions, transformations, and aggregations on the input data. Kafka Streaming enables
seamless streaming of data between various applications, making it an ideal choice for data
streaming and analytics.
By storing the data in a DBMS like PostgreSQL, it becomes easier to access and query the data for
further analysis or reporting. The project can be used for various use cases such as real-time data
analysis, fraud detection, and anomaly detection.

 Softwares used:

1. Apache Spark Streaming
2. Spark SQL
3. Docker
4 . Apache Kafka
5. PostgreSQL (or another DBMS of your choice)
6. Zookeeper